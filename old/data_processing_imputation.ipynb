{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jiekaitao/Documents/GitHub/FML_Group_Project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 23:11:39.075587: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-06 23:11:39.085604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743995499.098104 1074554 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743995499.101480 1074554 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-06 23:11:39.115866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_train shape: (5999, 1)\n",
      "x_train shape: (5999, 10000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from skimage.transform import rotate\n",
    "\n",
    "t_train_csv = pd.read_csv('./t_train_project.csv')\n",
    "x_train_csv = pd.read_csv('./x_train_project.csv')\n",
    "\n",
    "print(\"t_train shape:\", t_train_csv.shape)\n",
    "print(\"x_train shape:\", x_train_csv.shape)\n",
    "\n",
    "X = np.array(x_train_csv).reshape(-1, 100, 100, 1) # the data dimension with dimension \"1\" needs to be explicitly stated here for the CNN\n",
    "y = np.array(t_train_csv)\n",
    "\n",
    "# normalize everything for the CNN and MLP (from 0 to 1)\n",
    "X_normalized = X / 255.0\n",
    "\n",
    "X_train_normalized, X_val_normalized, y_train, y_val = train_test_split(X_normalized, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "# print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-= Training Fold 1/5 -=-=-=-=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiekaitao/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1743995505.157742 1074554 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14312 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-=-=-=- TRAINING FOR FOLD #1 -=-=-=-=-=-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743995506.589001 1074770 service.cc:148] XLA service 0x737d6800fad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743995506.589024 1074770 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti SUPER, Compute Capability 8.9\n",
      "2025-04-06 23:11:46.613386: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743995506.735587 1074770 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1743995510.076100 1074770 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-04-06 23:11:54.145411: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_108_0', 768 bytes spill stores, 720 bytes spill loads\n",
      "\n",
      "2025-04-06 23:11:54.162619: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_108_0', 228 bytes spill stores, 228 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 validation accuracy: 0.9583333134651184\n",
      "-=-=-=-= Training Fold 2/5 -=-=-=-=\n",
      "-=-=-=-=-=-=- TRAINING FOR FOLD #2 -=-=-=-=-=-\n",
      "Fold 2 validation accuracy: 0.9483333230018616\n",
      "-=-=-=-= Training Fold 3/5 -=-=-=-=\n",
      "-=-=-=-=-=-=- TRAINING FOR FOLD #3 -=-=-=-=-=-\n",
      "Fold 3 validation accuracy: 0.9708333611488342\n",
      "-=-=-=-= Training Fold 4/5 -=-=-=-=\n",
      "-=-=-=-=-=-=- TRAINING FOR FOLD #4 -=-=-=-=-=-\n",
      "Fold 4 validation accuracy: 0.9458333253860474\n",
      "-=-=-=-= Training Fold 5/5 -=-=-=-=\n",
      "-=-=-=-=-=-=- TRAINING FOR FOLD #5 -=-=-=-=-=-\n",
      "Fold 5 validation accuracy: 0.9674729108810425\n",
      "\n",
      "\n",
      "Cross-validation summary\n",
      "Score per fold [0.9583333134651184, 0.9483333230018616, 0.9708333611488342, 0.9458333253860474, 0.9674729108810425]\n",
      "Average validation accuracy 0.9581612467765808\n",
      "Average validation loss value 0.1765238106250763\n"
     ]
    }
   ],
   "source": [
    "# Without imputed data KFold testing\n",
    "# ~96.8% accuracy w/ 500 epochs\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "\n",
    "fold_i = 1\n",
    "for train_index, val_index in skf.split(X_normalized, y):\n",
    "    print(f'-=-=-=-= Training Fold {fold_i}/{5} -=-=-=-=')\n",
    "    X_train_fold, X_val_fold = X_normalized[train_index], X_normalized[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # model = Sequential([\n",
    "    #     Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)),\n",
    "    #     MaxPooling2D((2, 2)), # slide a 2x2 pooling filter to reduce overfitting\n",
    "    #     Conv2D(64, (3, 3), activation='relu'),\n",
    "    #     MaxPooling2D((2, 2)),\n",
    "    #     Conv2D(128, (3, 3), activation='relu'),\n",
    "    #     MaxPooling2D((2, 2)),\n",
    "    #     Conv2D(256, (3, 3), activation='relu'),\n",
    "    #     Flatten(), # flatten for the input layer of the MLP\n",
    "    #     # mlp part\n",
    "    #     Dense(256, activation='relu'),\n",
    "    #     Dropout(0.5),\n",
    "    #     Dense(128, activation='relu'),\n",
    "    #     Dropout(0.3),\n",
    "    #     Dense(64, activation='relu'),\n",
    "    #     Dropout(0.1),\n",
    "    #     Dense(10, activation='softmax') # output layer for the 10 possible classes\n",
    "    # ])\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', input_shape=(100, 100, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(512, (3, 3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # one-hot encode the labels fro the current fold\n",
    "    y_train_one_hot_encoded = np.zeros((y_train_fold.shape[0], 10)) # 2d data tensors w/ each row corresponding to one data sample's one hot encoding for calculating the difference (error) between model output and labels\n",
    "    y_val_one_hot_encoded = np.zeros((y_val_fold.shape[0], 10))\n",
    "\n",
    "    for i in range(len(y_train_fold)):\n",
    "        # y_train_fold[i] is shape (1,), so access element with [0]\n",
    "        y_train_one_hot_encoded[i, y_train_fold[i][0]] = 1\n",
    "\n",
    "    for i in range(len(y_val_fold)):\n",
    "         # y_val_fold[i] is shape (1,), so access element with [0]\n",
    "        y_val_one_hot_encoded[i, y_val_fold[i][0]] = 1\n",
    "\n",
    "    # print(y_val_one_hot_encoded)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(f'-=-=-=-=-=-=- TRAINING FOR FOLD #{fold_i} -=-=-=-=-=-')\n",
    "    history = model.fit(\n",
    "        X_train_fold, y_train_one_hot_encoded,\n",
    "        epochs=16,\n",
    "        batch_size=128,\n",
    "        validation_data=(X_val_fold, y_val_one_hot_encoded),\n",
    "        verbose=0 #reduce the console output\n",
    "    )\n",
    "\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_one_hot_encoded, verbose=0)\n",
    "    print(f\"Fold {fold_i} validation accuracy: {val_accuracy}\")\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    fold_i = fold_i + 1\n",
    "\n",
    "print(\"\\n\\nCross-validation summary\")\n",
    "print(f\"Score per fold {val_accuracies}\")\n",
    "print(f\"Average validation accuracy {np.mean(val_accuracies)}\")\n",
    "print(f\"Average validation loss value {np.mean(val_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_intensity(X_normalized, y, proportion=0.2, intensity_min = 0.5, intensity_max = 1.5):\n",
    "    # proportion = proportion of the X_normalized dataset to impute\n",
    "    # intensity_min is the minimum possible multiplication factors for the pixel intensity\n",
    "    # intensity_max is the max possible factor (upper range of gaussian)\n",
    "    num_samples_to_impute = int(np.floor(proportion*len(X_normalized)))\n",
    "    indices = np.random.choice(len(X_normalized), num_samples_to_impute, replace=False) # reaplce = False ensures sampels arent picked again (don't put back into possible selection)\n",
    "    \n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    for i in indices:\n",
    "        factor = np.random.uniform(intensity_min, intensity_max)\n",
    "        img_adjusted = np.clip(X_normalized[i] * factor, 0.0, 1.0) # resets to max 1 (normalizes it again)\n",
    "        X_augmented.append(img_adjusted)\n",
    "        y_augmented.append(y[i]) # add back in the original label\n",
    "    # output dimensions are (num_augmented, height, width, channels AKA 1 channel)\n",
    "    return np.array(X_augmented), np.array(y_augmented)\n",
    "\n",
    "def vary_rotation(X_normalized, y, proportion=0.2, angle_min=-30, angle_max=30):\n",
    "    num_samples_to_impute = int(np.floor(proportion*len(X_normalized)))\n",
    "    indices = np.random.choice(len(X_normalized), num_samples_to_impute, replace=False) # reaplce = False ensures sampels arent picked again (don't put back into possible selection)\n",
    "    \n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    for i in indices:\n",
    "        img_2d = X_normalized[i].reshape(100, 100)\n",
    "        angle = np.random.uniform(angle_min, angle_max)\n",
    "        # preserve_range True tells skimage to not resize the output image dimensions to fit\n",
    "        # the full rotated image, so we may **clip some data in the corners!!**\n",
    "        # mode=edge fills in the new space introduced by the rotation with the pixel values found at the edge\n",
    "        # preserve_range keeps the pixel values in [0,1] rather than [0,255]\n",
    "        rotated_img_2d = rotate(img_2d, angle, resize=False, mode='edge', preserve_range=True) # outputs (100,00)\n",
    "        X_augmented.append(rotated_img_2d.reshape(100, 100, 1)) # need to reshape to add that nested [] pixel intensity value (the dimension with size 1 in the tensor shape)\n",
    "        y_augmented.append(y[i]) # add back in the original label\n",
    "    # output dimensions are (num_augmented, height, width, channels AKA 1 channel)\n",
    "    return np.array(X_augmented), np.array(y_augmented)\n",
    "\n",
    "# X_intensity, y_intensity = vary_intensity(X_normalized, y)\n",
    "# X_rotation, y_rotation = vary_rotation(X_normalized, y)\n",
    "\n",
    "# X_augmented = np.concatenate((X_normalized, X_intensity, X_rotation), axis=0)\n",
    "# y_augmented = np.concatenate((y, y_intensity, y_rotation), axis=0)\n",
    "\n",
    "# # Index 6000 (inclusive) to the end are AUGMENTED data samples and should\n",
    "# # NOT BE INCLUDED IN THE TEST SET\n",
    "# print(\"X_augmented shape:\", X_augmented.shape)\n",
    "# print(\"y_augmented shape:\", y_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-= Training Fold 1/5 -=-=-=-=\n",
      "fold #1: Original training samples count: 4799\n",
      "fold #1: Original validation samples count: 1200\n",
      "fold #1: Generated 2399 samples by varying intensity\n",
      "fold #1: Generated 2399 samples by varying rotation\n",
      "fold #1 final training set size now has 9597 samples\n",
      "-=-=-=-=-=-=- TRAINING FOR FOLD #1 -=-=-=-=-=-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiekaitao/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold #1 validation accuracy 0.9649999737739563\n",
      "-=-=-=-= Training Fold 2/5 -=-=-=-=\n",
      "fold #2: Original training samples count: 4799\n",
      "fold #2: Original validation samples count: 1200\n",
      "fold #2: Generated 2399 samples by varying intensity\n",
      "fold #2: Generated 2399 samples by varying rotation\n",
      "fold #2 final training set size now has 9597 samples\n",
      "-=-=-=-=-=-=- TRAINING FOR FOLD #2 -=-=-=-=-=-\n",
      "fold #2 validation accuracy 0.9449999928474426\n",
      "-=-=-=-= Training Fold 3/5 -=-=-=-=\n",
      "fold #3: Original training samples count: 4799\n",
      "fold #3: Original validation samples count: 1200\n",
      "fold #3: Generated 2399 samples by varying intensity\n",
      "fold #3: Generated 2399 samples by varying rotation\n",
      "fold #3 final training set size now has 9597 samples\n",
      "-=-=-=-=-=-=- TRAINING FOR FOLD #3 -=-=-=-=-=-\n",
      "fold #3 validation accuracy 0.9783333539962769\n",
      "-=-=-=-= Training Fold 4/5 -=-=-=-=\n",
      "fold #4: Original training samples count: 4799\n",
      "fold #4: Original validation samples count: 1200\n",
      "fold #4: Generated 2399 samples by varying intensity\n",
      "fold #4: Generated 2399 samples by varying rotation\n",
      "fold #4 final training set size now has 9597 samples\n",
      "-=-=-=-=-=-=- TRAINING FOR FOLD #4 -=-=-=-=-=-\n",
      "fold #4 validation accuracy 0.9649999737739563\n",
      "-=-=-=-= Training Fold 5/5 -=-=-=-=\n",
      "fold #5: Original training samples count: 4800\n",
      "fold #5: Original validation samples count: 1199\n",
      "fold #5: Generated 2400 samples by varying intensity\n",
      "fold #5: Generated 2400 samples by varying rotation\n",
      "fold #5 final training set size now has 9600 samples\n",
      "-=-=-=-=-=-=- TRAINING FOR FOLD #5 -=-=-=-=-=-\n",
      "fold #5 validation accuracy 0.9641367793083191\n",
      "\n",
      "\n",
      "Cross-validation summary\n",
      "Score per fold [0.9649999737739563, 0.9449999928474426, 0.9783333539962769, 0.9649999737739563, 0.9641367793083191]\n",
      "Average validation accuracy 0.9634940147399902\n",
      "Average validation loss value 0.19306075870990752\n"
     ]
    }
   ],
   "source": [
    "# I was previously getting 99.6% accuracy.. there was some data leakage into the validation set\n",
    "\n",
    "# Now I think KFold testing is successful WITH imputed data for training, validating ONLY on original data\n",
    "# Making sure not to contaminate the validation data by generating augmentation\n",
    "# ONLY from the training split of each fold. (the \n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "\n",
    "fold_i = 1\n",
    "# skf.split iterates over the ORIGINAL data (X_normalized, y)\n",
    "# train_index and val_index refer to indices within the original X_normalized and y\n",
    "for train_index, val_index in skf.split(X_normalized, y):\n",
    "    print(f'-=-=-=-= Training Fold {fold_i}/{5} -=-=-=-=')\n",
    "\n",
    "    # using the values from the first code cell, get the the original training and validation splits for this fold\n",
    "    X_train_fold_orig, X_val_fold = X_normalized[train_index], X_normalized[val_index]\n",
    "    y_train_fold_orig, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    print(f\"fold #{fold_i}: Original training samples count: {len(X_train_fold_orig)}\")\n",
    "    print(f\"fold #{fold_i}: Original validation samples count: {len(X_val_fold)}\")\n",
    "\n",
    "    # Generate augmented data ONLY from the original TRAINING split of this fold\n",
    "    # We apply the augmentation functions *only* to X_train_fold_orig and y_train_fold_orig\n",
    "    # Using proportions relative to the size of this specific training fold\n",
    "\n",
    "    # the number of augmented training samples is floor(training split count * 0.5).. increase by increasing the last parameter\n",
    "    X_intensity_fold, y_intensity_fold = vary_intensity(X_train_fold_orig, y_train_fold_orig, proportion=0.5)\n",
    "    X_rotation_fold, y_rotation_fold = vary_rotation(X_train_fold_orig, y_train_fold_orig, proportion=0.5)\n",
    "\n",
    "    print(f\"fold #{fold_i}: Generated {len(X_intensity_fold)} samples by varying intensity\")\n",
    "    print(f\"fold #{fold_i}: Generated {len(X_rotation_fold)} samples by varying rotation\")\n",
    "\n",
    "    # Now combine the original training data with the augmented data\n",
    "    X_train_fold = np.concatenate((X_train_fold_orig, X_intensity_fold, X_rotation_fold), axis=0) # concatenate downward in data tensor shape (adding more sampels to the list up to down)\n",
    "    y_train_fold = np.concatenate((y_train_fold_orig, y_intensity_fold, y_rotation_fold), axis=0)\n",
    "\n",
    "    # During training, the model can get its initial \"graph traversal pathways\" biased if we only train with mostly original data at first\n",
    "    # so we should shuffle the original and augmented samples around\n",
    "    shuffle_indices = np.random.permutation(len(X_train_fold))\n",
    "    X_train_fold = X_train_fold[shuffle_indices]\n",
    "    y_train_fold = y_train_fold[shuffle_indices]\n",
    "\n",
    "    print(f\"fold #{fold_i} final training set size now has {X_train_fold.shape[0]} samples\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', input_shape=(100, 100, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(512, (3, 3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # one-hot encode the labels fro the current fold\n",
    "    # using the size of the COMBINED training labels\n",
    "    y_train_one_hot_encoded = np.zeros((y_train_fold.shape[0], 10)) # 2d data tensors w/ each row corresponding to one data sample's one hot encoding for calculating the difference (error) between model output and labels\n",
    "    # use the size of the original validation labels\n",
    "    y_val_one_hot_encoded = np.zeros((y_val_fold.shape[0], 10))\n",
    "\n",
    "    # now one hot encode the combined training labels\n",
    "    for i in range(len(y_train_fold)):\n",
    "        # y_train_fold[i] is shape (1,), so access element with [0]\n",
    "        y_train_one_hot_encoded[i, y_train_fold[i][0]] = 1\n",
    "\n",
    "    # one hot encode the original training labels\n",
    "    for i in range(len(y_val_fold)):\n",
    "         # y_val_fold[i] is shape (1,), so access element with [0]\n",
    "        y_val_one_hot_encoded[i, y_val_fold[i][0]] = 1\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(f'-=-=-=-=-=-=- TRAINING FOR FOLD #{fold_i} -=-=-=-=-=-')\n",
    "    # fit hte model to the combined (with non-augmented/original + fold-specific augmented) training data\n",
    "    history = model.fit(\n",
    "        X_train_fold, y_train_one_hot_encoded,\n",
    "        epochs=16,\n",
    "        batch_size=128,\n",
    "        # validates only on the original non-imputed dataset\n",
    "        validation_data=(X_val_fold, y_val_one_hot_encoded),\n",
    "        verbose=0 #reduce the console output\n",
    "    )\n",
    "\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_one_hot_encoded, verbose=0)\n",
    "    print(f\"fold #{fold_i} validation accuracy {val_accuracy}\")\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    fold_i = fold_i + 1\n",
    "\n",
    "print(\"\\n\\nCross-validation summary\")\n",
    "print(f\"Score per fold {val_accuracies}\")\n",
    "print(f\"Average validation accuracy {np.mean(val_accuracies)}\")\n",
    "print(f\"Average validation loss value {np.mean(val_losses)}\")\n",
    "\n",
    "# next up: test out increasing model complexity (parameter count) while increasing the model augmentation proportion (training data size)\n",
    "# in accordance with the neural scaling laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: sleep NUMBER[SUFFIX]...\n",
      "  or:  sleep OPTION\n",
      "Pause for NUMBER seconds.  SUFFIX may be 's' for seconds (the default),\n",
      "'m' for minutes, 'h' for hours or 'd' for days.  NUMBER need not be an\n",
      "integer.  Given two or more arguments, pause for the amount of time\n",
      "specified by the sum of their values.\n",
      "\n",
      "      --help        display this help and exit\n",
      "      --version     output version information and exit\n",
      "\n",
      "GNU coreutils online help: <https://www.gnu.org/software/coreutils/>\n",
      "Full documentation <https://www.gnu.org/software/coreutils/sleep>\n",
      "or available locally via: info '(coreutils) sleep invocation'\n"
     ]
    }
   ],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############\n",
    "# ## OLD STUFF ##\n",
    "# ###############\n",
    "\n",
    "\n",
    "\n",
    "# ###############\n",
    "# ## Train test train split\n",
    "# ###############\n",
    "\n",
    "\n",
    "# # 96.8% accuracy w/ 500 epochs\n",
    "\n",
    "# model = Sequential([\n",
    "#     Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)),\n",
    "#     MaxPooling2D((2, 2)), # slide a 2x2 pooling filter to reduce overfitting\n",
    "#     Conv2D(64, (3, 3), activation='relu'),\n",
    "#     MaxPooling2D((2, 2)),\n",
    "#     Conv2D(128, (3, 3), activation='relu'),\n",
    "#     MaxPooling2D((2, 2)),\n",
    "#     Conv2D(256, (3, 3), activation='relu'),\n",
    "#     Flatten(), # flatten for the input layer of the MLP\n",
    "#     # mlp part\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.1),\n",
    "#     Dense(10, activation='softmax') # output layer for the 10 possible classes\n",
    "# ])\n",
    "\n",
    "# y_train_one_hot_encoded = np.zeros((y_train.shape[0], 10)) # 2d data tensors w/ each row corresponding to one data sample's one hot encoding for calculating the difference (error) between model output and labels\n",
    "# y_val_one_hot_encoded = np.zeros((y_val.shape[0], 10))\n",
    "\n",
    "# for i in range(len(y_train)):\n",
    "#     y_train_one_hot_encoded[i, y_train[i]] = 1\n",
    "    \n",
    "# for i in range(len(y_val)):\n",
    "#     y_val_one_hot_encoded[i, y_val[i]] = 1\n",
    "\n",
    "# # print(y_val_one_hot_encoded)\n",
    "\n",
    "# y_train = y_train_one_hot_encoded\n",
    "# y_val = y_val_one_hot_encoded\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train_normalized, y_train,\n",
    "#     epochs=16, # 16 before\n",
    "#     batch_size=128,\n",
    "#     validation_data=(X_val_normalized, y_val),\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# val_loss, val_accuracy = model.evaluate(X_val_normalized, y_val)\n",
    "# print(\"validation accuracy:\", val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
